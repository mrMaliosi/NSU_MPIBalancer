#define _CRT_SECURE_NO_WARNINGS
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <time.h>
#include <stddef.h>
#include <pthread.h>

//Сижу я долго за компом, что затекла нога
//И нету мысли уж давно и слова в тоске опять
//Иванишкин говорит, что с кодом всё не то
//Какой же MPI шальной и всегда с ним всё не то..

//Не прессуй меня MPI, ой не прессуй...
//Без костыля и кода нееет..
//Пусть лажа в проде, её ты подебаж..
//Консоль наш крест и оберееег...

//Не прессуй меня MPI, ой не прессуй...
//Без костыля и кода нееет..
//Пусть лажа в проде, её ты подебажь..
//Расставлю я тебе обед..

//Снова костыли на коде, а за окном 4-ый час
//Сервер падает и стонет и VS горит опять..
//Логи наши в гигабайтах, а код всё равно горит.. 
//Прошу прости меня за все грехи мои..

//Не прессуй меня MPI, ой не прессуй...
//Без костыля и кода нееет..
//Пусть лажа в проде, её ты подебаж..
//Консоль наш крест и оберееег...

//Не прессуй меня MPI, ой не прессуй...
//Без костыля и кода нееет..
//Пусть лажа в проде, её ты подебаж..
//Расставлю я тебе обед..

#define FIRST_PROCESS 0
#define TASK_SIZE 100
#define MAX_PROCESSES_SIZE 2000
#define BIG_BOY 9999999999999999

typedef long long ll;

typedef struct Task
{
	ll id;
	ll type;
	ll n;
} Task;

MPI_Datatype MPI_Task;

Task TaskList_new[TASK_SIZE];
Task TaskList_ready[TASK_SIZE];
ll key = 1000000000;
ll offset = 1234567899;
ll cycle = 10000000000;

int recvcounts[MAX_PROCESSES_SIZE];
int displs[MAX_PROCESSES_SIZE];
int reports[MAX_PROCESSES_SIZE] = { 1 };

int start = 0;
int recvcount = 0;
int how_much_time_until_six = 0;

pthread_mutex_t work_mutex = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t  taskNeedSignal = PTHREAD_COND_INITIALIZER;
pthread_cond_t  taskDoneSignal = PTHREAD_COND_INITIALIZER;

void *FSB(void * arr) {
	
	for (;;) {
		int letter_from[1];
		MPI_Recv(letter_from, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, NULL);		//Получаем запрос 

		//Завершаем поток
		if (letter_from == -1) {
			break;
		}

		// Lock mutex and then wait for signal to relase mutex
		pthread_mutex_lock(&work_mutex);
		if ((how_much_time_until_six * 100) / TASK_SIZE > 20)					//Высчитываем процент оставшихся в пуле задач
		{
			int data_transmission_count = how_much_time_until_six / 2;
			MPI_Send(&data_transmission_count, 1, MPI_INT, letter_from[0], MPI_ANY_TAG, MPI_COMM_WORLD);
			MPI_Send(TaskList_ready + recvcount * sizeof(Task) - data_transmission_count * (sizeof(Task)), data_transmission_count, MPI_Task, letter_from[0], MPI_ANY_TAG, MPI_COMM_WORLD);
		}
		else {
			int data_transmission_count = -1;
			MPI_Send(&data_transmission_count, 1, MPI_INT, letter_from[0], MPI_ANY_TAG, MPI_COMM_WORLD);
		}
		pthread_mutex_unlock(&work_mutex);	
	}
	free(arr);
	pthread_exit(NULL);
}

void *execute_task(void * arr) {

	while (start < recvcount)
	{
		ll id = TaskList_ready[start].id;
		ll n = TaskList_ready[start].n;
		ll a = 0;
		for (ll i = 0; i < n; ++i) {
			a += 1;
		}
		printf("array[0]: %lld, array[1]: %lld, id: %lld, n: %lld\n", id, n, id, a);
		++start;
		pthread_mutex_lock(&work_mutex);
		how_much_time_until_six = recvcount - start;
		pthread_cond_signal(&taskDoneSignal);
		pthread_mutex_unlock(&work_mutex);
	}
	pthread_exit(NULL);
}

void *send_report(void* arr2) {
	int* function_array = (int*)arr2;
	int process = function_array[0];
	int process_size = function_array[1];
	
	for (;;)
	{
		pthread_mutex_lock(&work_mutex);
		pthread_cond_wait(&taskDoneSignal, &work_mutex);
		int percent = how_much_time_until_six * 100;
		pthread_mutex_unlock(&work_mutex);
		if ( percent / TASK_SIZE < 10 )					//Высчитываем процент оставшихся в пуле задач
		{

			int ask_to_process = rand() % process_size;			
			int flag = 1;
			for (int k = 0; k < process_size; ++k) {
				if (ask_to_process != process)
				{
					MPI_Send(&process, 1, MPI_INT, ask_to_process, MPI_ANY_TAG, MPI_COMM_WORLD);
					int data_transmission_count;
					MPI_Recv(&data_transmission_count, 1, MPI_INT, ask_to_process, MPI_ANY_TAG, MPI_COMM_WORLD, NULL);
					if (data_transmission_count != -1) {
						flag = 0;
						MPI_Recv(TaskList_ready + recvcount * (sizeof(Task)), data_transmission_count, MPI_Task, ask_to_process, MPI_ANY_TAG, MPI_COMM_WORLD, NULL);
						break;
					}
				}
				

				ask_to_process = (ask_to_process + 1) % process_size;
			}

			if (flag) {
				MPI_Barrier(MPI_COMM_WORLD);
				int it_is_time_to_stop = -1;
				MPI_Send(&it_is_time_to_stop, 1, MPI_INT, process, MPI_ANY_TAG, MPI_COMM_WORLD);
				break;
			}
		}
	}
	
	free(arr2);
	pthread_exit(NULL);
}

int main(int argc, char ** argv)
{
	for (int i = 0; i < MAX_PROCESSES_SIZE; ++i) {
		reports[i] = 1;
	}

	MPI_Init(&argc, &argv);
	int process, process_size;
	MPI_Comm_rank(MPI_COMM_WORLD, &process);				// Получаем номер конкретного процесса на котором запущена программа
	MPI_Comm_size(MPI_COMM_WORLD, &process_size);			// Получаем количество запущенных процессов
	if (process == FIRST_PROCESS) {
		printf("process_size: %d, %d\n", process_size, reports[100]);
	}


	MPI_Datatype MPI_Task;
	int blocklengths[3] = { 1, 1, 1 };
	MPI_Datatype types[3] = { MPI_LONG_LONG, MPI_LONG_LONG, MPI_LONG_LONG };
	MPI_Aint offsets[3];

	offsets[0] = offsetof(Task, id);
	offsets[1] = offsetof(Task, type);
	offsets[2] = offsetof(Task, n);

	MPI_Type_create_struct(3, blocklengths, offsets, types, &MPI_Task);
	MPI_Type_commit(&MPI_Task);


	for (int o = 0; o < 1; ++o) {
		start = 0;
		///////////////////Составляем список задач///////////////////
		if (process == FIRST_PROCESS) {
			for (int i = 0; i < TASK_SIZE; ++i) {
				TaskList_new[i].id = i;
				TaskList_new[i].type = 1;
				TaskList_new[i].n = key;
				//printf("i: %d, key: %lld, TaskList_new[i].n: %lld\n", i, key, TaskList_new[i].n);
				key = (key + offset) % cycle;
			}

			// Расчет количества элементов для каждого процесса
			int base_count = TASK_SIZE / process_size;
			int remainder = TASK_SIZE % process_size;

			// Расчет смещений и количества элементов для каждого процесса
			for (int i = 0; i < process_size; i++) {
				recvcounts[i] = base_count + (i < remainder ? 1 : 0);
				displs[i] = (i > 0) ? (displs[i - 1] + recvcounts[i - 1]) : 0;
			}
		}
		int base_count = TASK_SIZE / process_size;
		int remainder = TASK_SIZE % process_size;
		recvcount = base_count + (process < remainder ? 1 : 0);
		MPI_Scatterv(TaskList_new, recvcounts, displs, MPI_Task, TaskList_ready, recvcount, MPI_Task, FIRST_PROCESS, MPI_COMM_WORLD);

		pthread_t comrade_Malinovskii, comrade_Ivanishkin, comrade_Maliosi;
		int iret1, iret2, iret3;

		printf("process: %d, start: %d, recvcount: %d\n", process, start, recvcount);
		iret1 = pthread_create(&comrade_Maliosi, NULL, execute_task, NULL);
		++start;
		if (iret1) {
			fprintf(stderr, "Ошибка 1 - pthread_create() return code: %d\n", iret1);
			exit(EXIT_FAILURE);
		}

		int* arr2 = (int*)malloc(2 * sizeof(int));
		arr2[0] = process;
		arr2[1] = process_size;
		iret2 = pthread_create(&comrade_Malinovskii, NULL, send_report, (void*)arr2);
		if (iret2) {
			fprintf(stderr, "Ошибка 2 - pthread_create() return code: %d\n", iret2);
			exit(EXIT_FAILURE);
		}

		iret3 = pthread_create(&comrade_Ivanishkin, NULL, FSB, (void*)arr2);
		if (iret2) {
			fprintf(stderr, "Ошибка 3 - pthread_create() return code: %d\n", iret2);
			exit(EXIT_FAILURE);
		}

		void *result_Maliosi;
		pthread_join(comrade_Maliosi, &result_Maliosi);
		void *result_Malinovskii;
		pthread_join(comrade_Malinovskii, &result_Ivanishkin);
		void *result_Ivanishkin;
		pthread_join(comrade_Ivanishkin, &result_Ivanishkin);
			


	}

	MPI_Finalize();
	return 0;
}




